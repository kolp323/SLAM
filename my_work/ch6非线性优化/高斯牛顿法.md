高斯–牛顿法（Gauss–Newton method）是**用来求解非线性最小二乘问题**的一种迭代优化方法，主要用在模型参数拟合中，比如曲线拟合、相机位姿估计（BA 优化）、SLAM 中的后端优化等。
它其实是牛顿法的一个近似版本——用来处理**目标函数是平方和形式**的情况，这样可以避免计算 Hessian（海森矩阵）的复杂工作。

---

## 1. 问题背景

假设我们要最小化：

$$
\min_{\mathbf{x}} \ \frac{1}{2} \sum_{i=1}^m r_i(\mathbf{x})^2
$$

其中：

* $\mathbf{x}$ 是待优化的参数向量（n 维）
* $r_i(\mathbf{x})$ 是第 $i$ 个残差（观测值与预测值的差）
* m 通常 ≥ n

可以写成矩阵形式：

$$
F(\mathbf{x}) = \frac{1}{2} \| \mathbf{r}(\mathbf{x}) \|^2
$$

其中：

$$
\mathbf{r}(\mathbf{x}) = 
\begin{bmatrix}
r_1(\mathbf{x}) \\
r_2(\mathbf{x}) \\
\vdots \\
r_m(\mathbf{x})
\end{bmatrix}
$$

---

## 2. 经典牛顿法

牛顿法更新公式为：

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - [\nabla^2 F(\mathbf{x}_k)]^{-1} \nabla F(\mathbf{x}_k)
$$

* 梯度：

$$
\nabla F(\mathbf{x}) = J(\mathbf{x})^T \mathbf{r}(\mathbf{x})
$$

* Hessian（海森矩阵）：

$$
\nabla^2 F(\mathbf{x}) = J(\mathbf{x})^T J(\mathbf{x}) + \sum_{i=1}^m r_i(\mathbf{x}) \nabla^2 r_i(\mathbf{x})
$$

其中 $J(\mathbf{x})$ 是 **Jacobian 矩阵**（m×n）。

---

## 3. 高斯–牛顿法的近似

注意 Hessian 的第二项：

$$
\sum_{i=1}^m r_i(\mathbf{x}) \nabla^2 r_i(\mathbf{x})
$$

如果**残差较小**（或者接近最优解），这一项可以忽略，于是：

$$
\nabla^2 F(\mathbf{x}) \approx J^T J
$$

这样更新公式变成：

$$
(J^T J) \, \Delta \mathbf{x} = - J^T \mathbf{r}
$$

然后：

$$
\mathbf{x}_{k+1} = \mathbf{x}_k + \Delta \mathbf{x}
$$

这就是高斯–牛顿法的核心：

* 不计算完整 Hessian
* 用 $J^T J$ 近似 Hessian
* 解一个 **正规方程**（Normal Equation） 得到步长

---

## 4. 算法步骤

1. **初始化**：给定初值 $\mathbf{x}_0$
2. **计算残差向量** $\mathbf{r}(\mathbf{x}_k)$
3. **计算 Jacobian** $J(\mathbf{x}_k)$
4. **构造并求解正规方程**：

   $$
   (J^T J) \, \Delta \mathbf{x} = - J^T \mathbf{r}
   $$
5. **更新参数**：

   $$
   \mathbf{x}_{k+1} = \mathbf{x}_k + \Delta \mathbf{x}
   $$
6. **检查收敛条件**（步长小于阈值或残差变化小于阈值）
7. **重复**步骤 2–6 直到收敛

---

## 5. 特点

**优点**：

* 比普通牛顿法快（因为不算 Hessian 的二阶项）
* 适合非线性最小二乘问题
* 在接近最优解时收敛速度快（局部二次收敛）

**缺点**：

* 对初值敏感，差的初值可能导致发散
* 当 $J^T J$ 病态（接近奇异）时，求解可能不稳定（通常用 LM 方法改进）

---

## 6. 与 LM（Levenberg–Marquardt）关系

* 高斯–牛顿法在病态或非线性很强的情况下可能不收敛
* LM 方法在方程中加入阻尼项：

  $$
  (J^T J + \lambda I)\Delta x = - J^T r
  $$

  兼具梯度下降法和高斯–牛顿法的优点，更稳健

---

## 7. 一个简单例子

假设拟合模型：

$$
y = a e^{b x}
$$

观测数据为 $(x_i, y_i)$，残差：

$$
r_i(a, b) = y_i - a e^{b x_i}
$$

* 构造 $J$（对 a、b 分别求偏导）
* 用高斯–牛顿法迭代更新 $(a, b)$，直到收敛

---

权重矩阵 $W$ 的来源，其实来自**统计学里的加权最小二乘（Weighted Least Squares, WLS）**思想。
你的这段代码就是在做一个**带权的高斯–牛顿法**，而权重的选择是和测量噪声模型直接挂钩的。

---

## 1. 从普通最小二乘到加权最小二乘

**普通最小二乘 (OLS)** 目标函数：

$$
\min_x \ \frac12 \sum_{i=1}^N r_i^2
$$

假设：

* 每个残差 $r_i$ 的测量误差是独立同分布（i.i.d.）
* 方差相同 $\sigma^2$

在这种情况下，不用加权，直接最小化平方和就行。

---

**加权最小二乘 (WLS)**：
如果每个观测的方差不同（比如有的点噪声大，有的点噪声小），我们应该让高置信度的点对优化结果影响更大。

加权形式：

$$
\min_x \ \frac12 \sum_{i=1}^N w_i \, r_i^2
$$

矩阵写法：

$$
\min_x \ \frac12 \mathbf{r}^T W \mathbf{r}
$$

其中 $W$ 是一个 $N\times N$ 的对角矩阵：

$$
W = \mathrm{diag}(w_1, w_2, \dots, w_N)
$$

---

## 2. 权重和测量噪声的关系

假设测量噪声是高斯分布：

$$
\epsilon_i \sim \mathcal{N}(0, \sigma_i^2)
$$

那么最大似然估计告诉我们，应该最小化：

$$
\sum_{i=1}^N \frac{r_i^2}{\sigma_i^2}
$$

也就是说：

$$
w_i = \frac{1}{\sigma_i^2}
$$

**权重等于方差的倒数**，方差越小（更精准的测量），权重越大。

---

## 3. 你的代码里的做法

代码里：

```cpp
H += inv_sigma * inv_sigma * J * J.transpose();
b += -inv_sigma * inv_sigma * error * J;
```

* `inv_sigma` 是 $1/\sigma$（标准差的倒数）
* 因为权重 $w_i = 1/\sigma^2$，所以在公式里乘的是 `inv_sigma * inv_sigma`
* 这样做等价于在目标函数中：

  $$
  \min_x \ \frac12 \sum_{i=1}^N \frac{r_i^2}{\sigma^2}
  $$
* 如果每个点的噪声 $\sigma_i$ 不一样，可以给每个点不同的权重。

---

## 4. 总结

* **W 的来源**：来自**最大似然估计**，假设观测噪声服从高斯分布且方差已知。
* **为什么用 $1/\sigma$**：

  * 方差 $\sigma^2$ 反映测量不确定性
  * 方差越大 → 测量越不可靠 → 权重越小
  * 权重 $w_i = 1/\sigma_i^2$ 保证了高置信度的点影响更大

---


